{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlexTrack 2025 Demand Response Prediction Pipeline\n",
    "\n",
    "This notebook implements a multi-model ensemble pipeline for predicting demand response flags and capacities. It includes:\n",
    "- Feature engineering with site archetypes.\n",
    "- Global and archetype-specific models using LightGBM and XGBoost.\n",
    "- Ensembling and submission generation.\n",
    "\n",
    "Run cells sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-02T15:36:24.361367Z",
     "iopub.status.busy": "2025-11-02T15:36:24.360787Z",
     "iopub.status.idle": "2025-11-02T15:36:28.332477Z",
     "shell.execute_reply": "2025-11-02T15:36:28.331681Z",
     "shell.execute_reply.started": "2025-11-02T15:36:24.361342Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required packages (run this first in a fresh environment)\n",
    "!pip install pandas numpy scikit-learn scipy lightgbm xgboost --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T15:36:28.334041Z",
     "iopub.status.busy": "2025-11-02T15:36:28.333760Z",
     "iopub.status.idle": "2025-11-02T15:36:33.002232Z",
     "shell.execute_reply": "2025-11-02T15:36:33.001569Z",
     "shell.execute_reply.started": "2025-11-02T15:36:28.334019Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import confusion_matrix, mean_absolute_error, f1_score, mean_squared_error\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import warnings\n",
    "import os\n",
    "from scipy.stats import linregress\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Configuration & Setup\n",
    "Define global flags and load the training/test data from Kaggle or local paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T15:36:33.003501Z",
     "iopub.status.busy": "2025-11-02T15:36:33.002922Z",
     "iopub.status.idle": "2025-11-02T15:36:33.595218Z",
     "shell.execute_reply": "2025-11-02T15:36:33.594415Z",
     "shell.execute_reply.started": "2025-11-02T15:36:33.003465Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# --- 0. Configuration & Setup ---\n",
    "USE_CALIBRATION = False\n",
    "RUN_GRID_SEARCH = False # Keep this False unless you want to re-tune on the new, larger dataset\n",
    "\n",
    "# --- Data Loading ---\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    # Use the full training data\n",
    "    train_df = pd.read_csv('/kaggle/input/flextrack/flextrack-2025-training-data-v0.2.csv', encoding=\"utf-8\")\n",
    "    # This should be the new extended test set\n",
    "    test_df = pd.read_csv('/kaggle/input/flextrack/flextrack-2025-public-test-data-v0.3.csv', encoding=\"utf-8\") \n",
    "    sample_submission_df = pd.read_csv('/kaggle/input/flextrack/flextrack-2025-random-prediction-data-v0.2.csv', encoding=\"utf-8\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Kaggle paths not found. Falling back to local paths.\")\n",
    "    train_df = pd.read_csv('flextrack-2025-training-data-v0.2.csv')\n",
    "    test_df = pd.read_csv('flextrack-2025-public-test-data-v0.2.csv')\n",
    "    sample_submission_df = pd.read_csv('flextrack-2025-random-prediction-data-v0.2.csv')\n",
    "\n",
    "# --- Column Definitions ---\n",
    "COL_SITE = 'Site'\n",
    "COL_TIME = 'Timestamp_Local'\n",
    "COL_TEMP = 'Dry_Bulb_Temperature_C'\n",
    "COL_RAD = 'Global_Horizontal_Radiation_W/m2'\n",
    "COL_POWER = 'Building_Power_kW'\n",
    "COL_FLAG = 'Demand_Response_Flag'\n",
    "COL_CAP = 'Demand_Response_Capacity_kW'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Engineering & Selection Functions\n",
    "Define helper functions for rolling slopes, site archetypes, and comprehensive feature creation/selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T15:36:33.597081Z",
     "iopub.status.busy": "2025-11-02T15:36:33.596829Z",
     "iopub.status.idle": "2025-11-02T15:36:33.632115Z",
     "shell.execute_reply": "2025-11-02T15:36:33.631513Z",
     "shell.execute_reply.started": "2025-11-02T15:36:33.597065Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Site Archetypes: {'siteA': 'small', 'siteB': 'small', 'siteC': 'large', 'siteD': 'small', 'siteE': 'small', 'siteF': 'large', 'siteG': 'small', 'siteH': 'small', 'siteI': 'large', 'siteJ': 'large', 'siteK': 'large', 'siteL': 'large', 'siteM': 'large'}\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Feature Engineering & Selection Functions ---\n",
    "\n",
    "def calculate_rolling_slope(series):\n",
    "    x = np.arange(len(series))\n",
    "    valid_indices = ~np.isnan(series)\n",
    "    if np.sum(valid_indices) < 2: return np.nan\n",
    "    slope, _, _, _, _ = linregress(x[valid_indices], series[valid_indices])\n",
    "    return slope\n",
    "\n",
    "# NEW: Function to dynamically determine site archetypes\n",
    "def get_site_archetypes(df):\n",
    "    \"\"\"\n",
    "    Analyzes the full dataset to classify sites into archetypes based on power consumption.\n",
    "    \"\"\"\n",
    "    # Calculate the mean power consumption for each site\n",
    "    site_power_stats = df.groupby(COL_SITE)[COL_POWER].mean()\n",
    "    \n",
    "    # Define a threshold to separate small and large consumers\n",
    "    # A threshold of 100 kW seems appropriate based on your data description.\n",
    "    power_threshold = 60 \n",
    "    \n",
    "    archetype_map = {\n",
    "        site: 'large' if power > power_threshold else 'small'\n",
    "        for site, power in site_power_stats.items()\n",
    "    }\n",
    "    print(\"Generated Site Archetypes:\", archetype_map)\n",
    "    return archetype_map\n",
    "\n",
    "# MODIFIED: Get archetypes from the full training data once\n",
    "SITE_ARCHETYPE_MAP = get_site_archetypes(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T15:36:33.632930Z",
     "iopub.status.busy": "2025-11-02T15:36:33.632729Z",
     "iopub.status.idle": "2025-11-02T15:36:33.667070Z",
     "shell.execute_reply": "2025-11-02T15:36:33.666305Z",
     "shell.execute_reply.started": "2025-11-02T15:36:33.632915Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_features(df, baseline=False):\n",
    "    \"\"\"\n",
    "    MODIFIED: Now uses the dynamic SITE_ARCHETYPE_MAP for all sites.\n",
    "    \"\"\"\n",
    "    df[COL_TIME] = pd.to_datetime(df[COL_TIME])\n",
    "    df = df.set_index(COL_TIME)\n",
    "    \n",
    "    df['hour'] = df.index.hour\n",
    "    df['dayofweek'] = df.index.dayofweek\n",
    "    df['quarter'] = df.index.quarter\n",
    "    df['month'] = df.index.month\n",
    "    df['dayofyear'] = df.index.dayofyear\n",
    "    df['is_weekend'] = (df.index.dayofweek >= 5).astype(int)\n",
    "    \n",
    "    lags = [1, 2, 4, 8, 16, 24, 48, 56, 96, 144]\n",
    "    for lag in lags:\n",
    "        df[f'power_lag_{lag}'] = df.groupby(COL_SITE)[COL_POWER].shift(lag)\n",
    "        df[f'temp_lag_{lag}'] = df.groupby(COL_SITE)[COL_TEMP].shift(lag)\n",
    "        \n",
    "    windows = [4, 12, 24, 48, 56, 96, 144]\n",
    "    for window in windows:\n",
    "        df[f'power_roll_mean_{window}'] = df.groupby(COL_SITE)[COL_POWER].transform(lambda x: x.rolling(window).mean())\n",
    "        df[f'power_roll_std_{window}'] = df.groupby(COL_SITE)[COL_POWER].transform(lambda x: x.rolling(window).std())\n",
    "        \n",
    "    comfort_temp = 18.0\n",
    "    df['heating_degree'] = np.maximum(0, comfort_temp - df[COL_TEMP])\n",
    "    df['cooling_degree'] = np.maximum(0, df[COL_TEMP] - comfort_temp)\n",
    "    \n",
    "    df['temp_rad_interaction'] = df[COL_TEMP] * df[COL_RAD]\n",
    "    df['temp_squared'] = df[COL_TEMP] ** 2\n",
    "    df['power_diff_1'] = df.groupby(COL_SITE)[COL_POWER].diff(1)\n",
    "    df['power_cumsum_4'] = df.groupby(COL_SITE)[COL_POWER].transform(lambda x: x.rolling(4).sum())\n",
    "    \n",
    "    df['is_business_hours'] = ((df['hour'] >= 7) & (df['hour'] <= 18) & (df['is_weekend'] == 0)).astype(int)\n",
    "    df['is_peak_hours'] = ((df['hour'] >= 13) & (df['hour'] <= 20)).astype(int)\n",
    "    df['is_winter'] = df['month'].isin([6, 7, 8]).astype(int)\n",
    "    df['is_summer'] = df['month'].isin([12, 1, 2]).astype(int)\n",
    "    df['is_heating_likely'] = ((df['is_winter'] == 1) & ((df['is_weekend'] == 1) | (df['heating_degree'] > 0))).astype(int)\n",
    "    \n",
    "    # --- MODIFIED: Archetype Logic ---\n",
    "    df['site_archetype'] = df[COL_SITE].map(SITE_ARCHETYPE_MAP)\n",
    "    all_archetypes = ['small', 'large'] # Our new archetypes\n",
    "    df['site_archetype'] = pd.Categorical(df['site_archetype'], categories=all_archetypes)\n",
    "    df = pd.get_dummies(df, columns=['site_archetype'], prefix='archetype')\n",
    "    \n",
    "    for archetype in ['archetype_small', 'archetype_large']:\n",
    "        if archetype in df.columns:\n",
    "            df[f'{archetype}_temp_interaction'] = df[archetype] * df[COL_TEMP]\n",
    "            df[f'{archetype}_power_interaction'] = df[archetype] * df[COL_POWER]\n",
    "    # --- END MODIFIED Archetype Logic ---\n",
    "\n",
    "    df['power_diff_96'] = df.groupby(COL_SITE)[COL_POWER].diff(96)\n",
    "    df['temp_diff_96'] = df.groupby(COL_SITE)[COL_TEMP].diff(96)\n",
    "    df['power_vs_roll_mean_96'] = df[COL_POWER] - df[f'power_roll_mean_96']\n",
    "\n",
    "    if(not baseline):\n",
    "        df['day_of_year_sin'] = np.sin(2 * np.pi * df['dayofyear'] / 365)\n",
    "        df['day_of_year_cos'] = np.cos(2 * np.pi * df['dayofyear'] / 365)\n",
    "        df['is_lunch_hour'] = ((df['hour'] >= 12) & (df['hour'] < 13) & (df['is_weekend'] == 0)).astype(int)\n",
    "        df['is_overnight'] = ((df['hour'] >= 22) | (df['hour'] < 5)).astype(int)\n",
    "        df['temp_deviation_from_comfort'] = np.abs(df[COL_TEMP] - 18.0)\n",
    "        df['temp_bins'] = pd.cut(df[COL_TEMP], bins=[-np.inf, 10, 18, 25, np.inf], labels=['cold', 'mild', 'warm', 'hot'])\n",
    "        for span in [8, 24]:\n",
    "            df[f'power_ewm_span_{span}'] = df.groupby(COL_SITE)[COL_POWER].transform(lambda x: x.ewm(span=span, adjust=False).mean())\n",
    "            df[f'temp_ewm_span_{span}'] = df.groupby(COL_SITE)[COL_TEMP].transform(lambda x: x.ewm(span=span, adjust=False).mean())\n",
    "        df[f'power_slope_4'] = df.groupby(COL_SITE)[COL_POWER].transform(lambda x: x.rolling(window=4, min_periods=2).apply(calculate_rolling_slope, raw=False))\n",
    "        df['temp_x_hour_sin'] = df[COL_TEMP] * np.sin(2 * np.pi * df['hour'] / 24)\n",
    "        df['temp_x_hour_cos'] = df[COL_TEMP] * np.cos(2 * np.pi * df['hour'] / 24)\n",
    "        df['temp_x_power_lag_96'] = df[COL_TEMP] * df['power_lag_96']\n",
    "        df = pd.get_dummies(df, columns=['temp_bins'], prefix='temp_bins')\n",
    "        df['day_of_month'] = df.index.day\n",
    "        df['week_of_year'] = df.index.isocalendar().week.astype(int)\n",
    "        df['time_of_day_minute'] = df.index.hour * 60 + df.index.minute\n",
    "        df['time_of_day_fraction'] = df['time_of_day_minute'] / 1440.0\n",
    "        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "        df['day_of_week_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
    "        df['day_of_week_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "        df['is_weekday'] = (df['dayofweek'] < 5).astype(int)\n",
    "        df['is_work_hours'] = ((df['hour'] >= 8) & (df['hour'] < 18) & (df['is_weekend'] == 0)).astype(int)\n",
    "        df['is_morning_ramp_up'] = ((df['hour'] >= 6) & (df['hour'] < 9) & (df['is_weekend'] == 0)).astype(int)\n",
    "        df['is_evening_ramp_down'] = ((df['hour'] >= 17) & (df['hour'] < 20) & (df['is_weekend'] == 0)).astype(int)\n",
    "        df['hour_of_week'] = df['dayofweek'] * 24 + df['hour']\n",
    "        df['is_first_work_hour'] = ((df['hour'] == 8) & (df['is_weekend'] == 0)).astype(int)\n",
    "        df['is_last_work_hour'] = ((df['hour'] == 17) & (df['is_weekend'] == 0)).astype(int)\n",
    "        df['season'] = (df['month'] % 12 + 3) // 3\n",
    "        conditions = [ (df['is_work_hours'] == 1), ((df['is_weekday'] == 1) & (df['is_work_hours'] == 0)), ((df['is_weekend'] == 1) & (df['hour'].between(8, 18))), ((df['is_weekend'] == 1) & ~(df['hour'].between(8, 18))) ]\n",
    "        choices = ['Workday_Hours', 'Workday_OffHours', 'Weekend_Day', 'Weekend_Night']\n",
    "        df['occupancy_state'] = np.select(conditions, choices, default='Unknown')\n",
    "        df['temp_cubed'] = df[COL_TEMP] ** 3\n",
    "        df['radiation_is_zero'] = (df[COL_RAD] == 0).astype(int)\n",
    "        df['radiation_sqrt'] = np.sqrt(df[COL_RAD])\n",
    "        df['is_mild_temp_workday'] = ((df[COL_TEMP] > 19) & (df[COL_TEMP] < 23) & (df['is_work_hours'] == 1)).astype(int)\n",
    "        df['is_extreme_heat_workday'] = ((df[COL_TEMP] > 30) & (df['is_work_hours'] == 1)).astype(int)\n",
    "        df['is_extreme_cold_workday'] = ((df[COL_TEMP] < 10) & (df['is_work_hours'] == 1)).astype(int)\n",
    "        for window in [4, 8, 12, 24, 48, 96, 288, 672]:\n",
    "            df[f'temp_roll_mean_{window}'] = df.groupby(COL_SITE)[COL_TEMP].transform(lambda x: x.rolling(window, min_periods=min(window//4, 1)).mean())\n",
    "            df[f'temp_roll_std_{window}'] = df.groupby(COL_SITE)[COL_TEMP].transform(lambda x: x.rolling(window, min_periods=min(window//4, 1)).std())\n",
    "        for window in [24, 96]:\n",
    "            df[f'temp_roll_min_{window}'] = df.groupby(COL_SITE)[COL_TEMP].transform(lambda x: x.rolling(window, min_periods=1).min())\n",
    "            df[f'temp_roll_max_{window}'] = df.groupby(COL_SITE)[COL_TEMP].transform(lambda x: x.rolling(window, min_periods=1).max())\n",
    "            df[f'temp_roll_range_{window}'] = df[f'temp_roll_max_{window}'] - df[f'temp_roll_min_{window}']\n",
    "        for diff_period in [1, 4, 24, 96]:\n",
    "            if f'temp_diff_{diff_period}' not in df.columns: df[f'temp_diff_{diff_period}'] = df.groupby(COL_SITE)[COL_TEMP].diff(diff_period)\n",
    "            df[f'rad_diff_{diff_period}'] = df.groupby(COL_SITE)[COL_RAD].diff(diff_period)\n",
    "        for window in [4, 12, 24, 48, 96, 144]:\n",
    "            df[f'power_roll_min_{window}'] = df.groupby(COL_SITE)[COL_POWER].transform(lambda x: x.rolling(window, min_periods=1).min())\n",
    "            df[f'power_roll_max_{window}'] = df.groupby(COL_SITE)[COL_POWER].transform(lambda x: x.rolling(window, min_periods=1).max())\n",
    "            df[f'power_roll_median_{window}'] = df.groupby(COL_SITE)[COL_POWER].transform(lambda x: x.rolling(window, min_periods=1).median())\n",
    "        for window in [24, 96]:\n",
    "            df[f'power_roll_skew_{window}'] = df.groupby(COL_SITE)[COL_POWER].transform(lambda x: x.rolling(window, min_periods=4).skew())\n",
    "            df[f'power_roll_kurt_{window}'] = df.groupby(COL_SITE)[COL_POWER].transform(lambda x: x.rolling(window, min_periods=4).kurt())\n",
    "        for diff_period in [1, 4, 24, 96, 672]:\n",
    "            if f'power_diff_{diff_period}' not in df.columns: df[f'power_diff_{diff_period}'] = df.groupby(COL_SITE)[COL_POWER].diff(diff_period)\n",
    "            df[f'power_pct_change_{diff_period}'] = df.groupby(COL_SITE)[COL_POWER].pct_change(diff_period)\n",
    "        df['occupancy_state_lag'] = df.groupby(COL_SITE)['occupancy_state'].shift(1)\n",
    "        df['state_changed'] = (df['occupancy_state'] != df['occupancy_state_lag']).astype(int)\n",
    "        df['is_work_hours_lag'] = df.groupby(COL_SITE)['is_work_hours'].shift(1)\n",
    "        df['is_transition_to_work'] = ((df['is_work_hours'] == 1) & (df['is_work_hours_lag'] == 0)).astype(int)\n",
    "        df['is_transition_from_work'] = ((df['is_work_hours'] == 0) & (df['is_work_hours_lag'] == 1)).astype(int)\n",
    "        df['time_since_last_transition'] = df.groupby([COL_SITE, df['state_changed'].cumsum()]).cumcount()\n",
    "        df = pd.get_dummies(df, columns=['occupancy_state'], prefix='state')\n",
    "        df = df.drop(columns=['occupancy_state_lag'])\n",
    "        df['temp_x_is_work_hours'] = df[COL_TEMP] * df['is_work_hours']\n",
    "        df['temp_x_is_weekend'] = df[COL_TEMP] * df['is_weekend']\n",
    "        df['rad_x_is_work_hours'] = df[COL_RAD] * df['is_work_hours']\n",
    "        df['rad_x_is_weekend'] = df[COL_RAD] * df['is_weekend']\n",
    "        df['rad_x_hour_sin'] = df[COL_RAD] * df['hour_sin']\n",
    "        df['rad_x_hour_cos'] = df[COL_RAD] * df['hour_cos']\n",
    "        df['effective_temp_load'] = df[COL_TEMP] + 0.01 * df[COL_RAD]\n",
    "        df['temp_x_season'] = df[COL_TEMP] * df['season']\n",
    "        df['temp_x_month_sin'] = df[COL_TEMP] * df['month_sin']\n",
    "        df['temp_x_month_cos'] = df[COL_TEMP] * df['month_cos']\n",
    "        df['hour_x_power_roll_mean_24'] = df['hour'] * df['power_roll_mean_24']\n",
    "        df['power_lag_96_x_dayofweek'] = df['power_lag_96'] * df['dayofweek']\n",
    "        df['rad_x_power_lag_96'] = df[COL_RAD] * df['power_lag_96']\n",
    "        df['temp_x_power_roll_mean_12'] = df[COL_TEMP] * df['power_roll_mean_12']\n",
    "        for window in [12, 24, 48]:\n",
    "            df[f'power_roll_sum_{window}'] = df.groupby(COL_SITE)[COL_POWER].transform(lambda x: x.rolling(window, min_periods=1).sum())\n",
    "            df[f'temp_roll_sum_{window}'] = df.groupby(COL_SITE)[COL_TEMP].transform(lambda x: x.rolling(window, min_periods=1).sum())\n",
    "            df[f'rad_roll_sum_{window}'] = df.groupby(COL_SITE)[COL_RAD].transform(lambda x: x.rolling(window, min_periods=1).sum())\n",
    "        if 'power_roll_mean_96' in df.columns and 'power_roll_max_96' in df.columns:\n",
    "            df['power_vs_daily_avg'] = df[COL_POWER] / (df['power_roll_mean_96'] + 1e-6)\n",
    "            df['power_vs_daily_max'] = df[COL_POWER] / (df['power_roll_max_96'] + 1e-6)\n",
    "        for window in [24, 96]:\n",
    "            mean_col = f'power_roll_mean_{window}'\n",
    "            std_col = f'power_roll_std_{window}'\n",
    "            if mean_col in df.columns and std_col in df.columns:\n",
    "                df[f'power_zscore_{window}'] = (df[COL_POWER] - df[mean_col]) / (df[std_col] + 1e-6)\n",
    "        for window in [96]:\n",
    "            df[f'power_percentile_{window}'] = df.groupby(COL_SITE)[COL_POWER].transform(\n",
    "                lambda x: x.rolling(window, min_periods=4).apply(\n",
    "                    lambda y: (y.iloc[-1] <= y).mean() if len(y) > 0 else np.nan, raw=False))\n",
    "            \n",
    "    return df.reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modular Training & Prediction Pipeline\n",
    "The core function for training LightGBM/XGBoost classifiers and regressors, with optional grid search and ensemble predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T15:36:33.668055Z",
     "iopub.status.busy": "2025-11-02T15:36:33.667876Z",
     "iopub.status.idle": "2025-11-02T15:36:33.690957Z",
     "shell.execute_reply": "2025-11-02T15:36:33.690224Z",
     "shell.execute_reply.started": "2025-11-02T15:36:33.668042Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_and_select_features(train_df, test_df):\n",
    "    print(\"  - Stage 1: Creating baseline and full feature sets...\")\n",
    "    train_df_baseline, test_df_baseline = create_features(train_df.copy(), baseline=True), create_features(test_df.copy(), baseline=True)\n",
    "    train_df_full, test_df_full = create_features(train_df.copy(), baseline=False), create_features(test_df.copy(), baseline=False)\n",
    "    train_cols = train_df_full.columns\n",
    "    test_df_full, test_df_baseline = test_df_full.reindex(columns=train_cols, fill_value=0), test_df_baseline.reindex(columns=train_df_baseline.columns, fill_value=0)\n",
    "\n",
    "    print(\"  - Stage 2: Selecting stable baseline features via correlation...\")\n",
    "    features_to_exclude = [COL_SITE, COL_TIME, COL_FLAG, COL_CAP, 'demand_response_flag_mapped']\n",
    "    baseline_features_raw = [col for col in train_df_baseline.columns if col not in features_to_exclude]\n",
    "    corr_matrix = train_df_baseline[baseline_features_raw].corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop_corr = [column for column in upper.columns if any(upper[column] > 0.98)]\n",
    "    stable_baseline_features = [f for f in baseline_features_raw if (f not in to_drop_corr) and (f not in ['power_roll_std_56', 'power_roll_mean_144'])]\n",
    "    print(f\"    Stable baseline feature count: {len(stable_baseline_features)}\")\n",
    "\n",
    "    print(\"  - Stage 3: Building specialized feature lists for models...\")\n",
    "    CLASSIFIER_FEATURES_TO_REMOVE = ['month', 'power_roll_mean_96', 'power_roll_std_56', 'power_roll_mean_144', 'power_lag_96', 'temp_lag_48', 'power_roll_std_4', 'temp_lag_144', 'archetype_large', 'archetype_small']\n",
    "    CLASSIFIER_FEATURES_TO_ADD = []\n",
    "    CLASSIFIER_FEATURES_TO_ADD2_ = ['hour_cos', 'day_of_week_cos', 'is_last_work_hour', 'is_mild_temp_workday', 'temp_roll_min_24', 'rad_diff_1', 'rad_diff_4', 'is_transition_to_work', 'is_transition_from_work', 'state_Workday_OffHours', 'rad_roll_sum_48', 'power_zscore_96']\n",
    "    \n",
    "    REGRESSOR_FEATURES_TO_REMOVE = [\"archetype_small_power_interaction\", \"Dry_Bulb_Temperature_C\", 'power_roll_std_56', 'power_roll_mean_144']\n",
    "    REGRESSOR_FEATURES_TO_REMOVE_2 = ['Dry_Bulb_Temperature_C', 'month', 'temp_lag_48', 'power_lag_56', 'power_lag_144', 'heating_degree', 'archetype_small_temp_interaction', 'archetype_small_x_dayofweek', 'archetype_small_x_heating_degree', 'power_diff_96', 'power_vs_roll_mean_96', 'is_first_work_hour', 'is_extreme_cold_workday', 'temp_roll_std_24', 'temp_roll_std_96', 'temp_diff_1', 'power_roll_kurt_96', 'power_pct_change_672', 'is_transition_from_work', 'state_Weekend_Night', 'temp_x_is_work_hours', 'rad_x_is_work_hours', 'rad_x_is_weekend', 'temp_x_season', 'temp_x_month_cos', 'hour_x_power_roll_mean_24', 'power_lag_96_x_dayofweek', 'rad_x_power_lag_96', 'rad_roll_sum_12', 'rad_roll_sum_24', 'power_vs_daily_avg', 'power_zscore_96']\n",
    "    REGRESSOR_FEATURES_TO_ADD = ['temp_ewm_span_24', 'temp_x_power_lag_96', 'temp_x_hour_sin', 'archetype_large_power_interaction', 'power_ewm_span_8', 'temp_bins_warm']\n",
    "    REGRESSOR_FEATURES_TO_ADD2_ = ['temp_roll_std_8', 'temp_roll_min_24', 'temp_diff_24', 'power_pct_change_4', 'temp_x_is_weekend', 'rad_x_hour_sin', 'power_vs_daily_max', 'temp_roll_std_4', 'rad_diff_4']\n",
    "    \n",
    "    features_clf = [f for f in stable_baseline_features if f not in CLASSIFIER_FEATURES_TO_REMOVE]\n",
    "    for f in (CLASSIFIER_FEATURES_TO_ADD + CLASSIFIER_FEATURES_TO_ADD2_):\n",
    "        if f not in features_clf and f in train_df_full.columns: features_clf.append(f)\n",
    "    features_reg = [f for f in stable_baseline_features if f not in REGRESSOR_FEATURES_TO_REMOVE]\n",
    "    for f in (REGRESSOR_FEATURES_TO_ADD + REGRESSOR_FEATURES_TO_ADD2_):\n",
    "        if f not in features_reg and f in train_df_full.columns: features_reg.append(f)\n",
    "    return train_df_full, test_df_full, features_clf, features_reg\n",
    "\n",
    "\n",
    "# --- 2. Modular Training & Prediction Pipeline ---\n",
    "# This function remains largely the same, as it was already well-structured.\n",
    "def train_and_predict_pipeline(train_df, test_df, features_clf, features_reg, run_grid_search=False, params=None):\n",
    "    # ... [Your train_and_predict_pipeline function copied verbatim] ...\n",
    "    # This function is well-designed and does not need major changes.\n",
    "    print(\"  - Starting training pipeline...\")\n",
    "    flag_mapping = {-1: 0, 0: 1, 1: 2}\n",
    "    train_df['demand_response_flag_mapped'] = train_df[COL_FLAG].map(flag_mapping)\n",
    "    # Using a simple time split. For single-site models, this is fine.\n",
    "    #split_date = train_df[COL_TIME].quantile(0.8, interpolation='nearest')\n",
    "    split_date = train_df[COL_TIME].max() - pd.Timedelta(days=31)\n",
    "    train_split, val_split = train_df[train_df[COL_TIME] < split_date], train_df[train_df[COL_TIME] >= split_date]\n",
    "    X_train_clf, y_train_flag = train_split[features_clf], train_split['demand_response_flag_mapped']\n",
    "    X_val_clf, y_val_flag_mapped = val_split[features_clf], val_split['demand_response_flag_mapped']\n",
    "    \n",
    "    if run_grid_search:\n",
    "        print(\"    EXECUTING GRID SEARCH...\")\n",
    "        # Classifier Search\n",
    "        clf_param_grid = {'n_estimators': [1500], 'learning_rate': [0.01, 0.02, 0.03], 'num_leaves': [31, 41, 51, 61], 'colsample_bytree': [0.8], 'min_child_samples': [20], 'reg_alpha': [0.1, 0.5], 'reg_lambda': [0.1, 0.5]}\n",
    "        lgbm_clf = lgb.LGBMClassifier(objective='multiclass', num_class=3, random_state=42, verbosity=-1, device=\"gpu\", class_weight='balanced')\n",
    "        grid_search_clf = GridSearchCV(estimator=lgbm_clf, param_grid=clf_param_grid, scoring='f1_macro', cv=TimeSeriesSplit(n_splits=3), n_jobs=-1, verbose=2).fit(X_train_clf, y_train_flag)\n",
    "        best_clf_params = grid_search_clf.best_params_\n",
    "        print(f\"    Best CLF Params Found: {best_clf_params}\")\n",
    "\n",
    "        # Regressor Search\n",
    "        train_reg_mask = train_split[COL_FLAG] != 0\n",
    "        X_train_reg, y_train_reg = train_split[train_reg_mask][features_reg], train_split[train_reg_mask][COL_CAP]\n",
    "        reg_param_grid = {'n_estimators': [1500], 'learning_rate': [0.01, 0.03, 0.05], 'num_leaves': [40, 50, 60, 80], 'colsample_bytree': [0.8], 'min_child_samples': [20], 'reg_alpha': [0.1, 0.5], 'reg_lambda': [0.1, 0.5]}\n",
    "        lgbm_reg = lgb.LGBMRegressor(objective='regression_l1', random_state=42, verbosity=-1, device=\"gpu\")\n",
    "        grid_search_reg = GridSearchCV(estimator=lgbm_reg, param_grid=reg_param_grid, scoring='neg_mean_absolute_error', cv=TimeSeriesSplit(n_splits=3), n_jobs=-1, verbose=2).fit(X_train_reg, y_train_reg)\n",
    "        best_reg_params = grid_search_reg.best_params_\n",
    "        print(f\"    Best REG Params Found: {best_reg_params}\")\n",
    "    else:\n",
    "        print(\"    Skipping GridSearchCV. Using pre-defined best parameters.\")\n",
    "        if params:\n",
    "            best_clf_params = params[\"CLF\"]\n",
    "            best_reg_params = params[\"REG\"]\n",
    "        else: # Default fallback\n",
    "            best_clf_params = {'colsample_bytree': 0.8, 'learning_rate': 0.02, 'min_child_samples': 20, 'n_estimators': 1500, 'num_leaves': 41, 'reg_alpha': 0.5, 'reg_lambda': 1.0}\n",
    "            best_reg_params= {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'min_child_samples': 20, 'n_estimators': 1500, 'num_leaves': 60, 'reg_alpha': 0.5, 'reg_lambda': 0.5}\n",
    "        print(f\"    Using CLF Params: {best_clf_params}\")\n",
    "        print(f\"    Using REG Params: {best_reg_params}\")\n",
    "\n",
    "    print(\"    Training final models on full data...\")\n",
    "    clf_params = {**best_clf_params, 'objective': 'multiclass', 'num_class': 3, 'random_state': 42, 'verbosity': -1, 'device': \"gpu\", 'class_weight': 'balanced'}\n",
    "    final_classifier = lgb.LGBMClassifier(**clf_params).fit(train_df[features_clf], train_df['demand_response_flag_mapped'], eval_set=[(X_val_clf, y_val_flag_mapped)], eval_metric='multi_logloss', callbacks=[lgb.early_stopping(250, verbose=False)])\n",
    "    clf_params_xb = {\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 3,\n",
    "        'n_estimators': best_clf_params.get('n_estimators', 2500),\n",
    "        'learning_rate': best_clf_params.get('learning_rate', 0.026),\n",
    "        'max_depth': best_clf_params.get('num_leaves', 29) // 2,  # approx depth\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'reg_alpha': best_clf_params.get('reg_alpha', 0.5),\n",
    "        'reg_lambda': best_clf_params.get('reg_lambda', 0.5),\n",
    "        'random_state': 42,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'early_stopping_rounds': 250,\n",
    "        'tree_method': 'gpu_hist',      # ← Critical for GPU\n",
    "        'predictor': 'gpu_predictor'    # ← Faster GPU inference (optional but recommended)\n",
    "    }\n",
    "    final_classifier_xb = XGBClassifier(**clf_params_xb)\n",
    "    final_classifier_xb.fit(\n",
    "        train_df[features_clf],\n",
    "        train_df['demand_response_flag_mapped'],\n",
    "        eval_set=[(X_val_clf, y_val_flag_mapped)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    train_reg_mask, val_reg_mask = train_df[COL_FLAG] != 0, val_split[COL_FLAG] != 0\n",
    "    X_full_train_reg, y_full_train_reg = train_df[train_reg_mask][features_reg], train_df[train_reg_mask][COL_CAP]\n",
    "    X_val_reg, y_val_reg_true = val_split[val_reg_mask][features_reg], val_split[val_reg_mask][COL_CAP]\n",
    "    reg_params = {**best_reg_params, 'objective': 'huber',  \"boosting_type\" : 'dart', \"random_state\" : 155, 'verbosity': -1, 'device': \"gpu\"}\n",
    "    final_regressor = lgb.LGBMRegressor(**reg_params).fit(X_full_train_reg, y_full_train_reg, eval_set=[(X_val_reg, y_val_reg_true)], eval_metric='mae', callbacks=[lgb.early_stopping(250, verbose=False)])\n",
    "\n",
    "    reg_params_xb = {\n",
    "        'objective': 'reg:absoluteerror',  # or 'reg:squarederror'\n",
    "        'n_estimators': best_reg_params.get('n_estimators', 10000),\n",
    "        'learning_rate': best_reg_params.get('learning_rate', 0.0175),\n",
    "        'max_depth': 6,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'reg_alpha': 0.3,\n",
    "        'reg_lambda': 0.2,\n",
    "        'random_state': 155,\n",
    "        'eval_metric': 'mae',\n",
    "        'early_stopping_rounds': 250,\n",
    "        'tree_method': 'gpu_hist',        # ← GPU\n",
    "        'predictor': 'gpu_predictor'\n",
    "    }\n",
    "    \n",
    "    final_regressor_xb = XGBRegressor(**reg_params_xb)\n",
    "    final_regressor_xb.fit(\n",
    "        X_full_train_reg, y_full_train_reg,\n",
    "        eval_set=[(X_val_reg, y_val_reg_true)],\n",
    "        verbose=False\n",
    "    )    \n",
    "    # --- GENERATE ENSEMBLED PREDICTIONS ---\n",
    "    print(\"    Generating ensemble predictions...\")\n",
    "    \n",
    "    # Classifier: average probabilities\n",
    "    flag_probs_lgb = final_classifier.predict_proba(test_df[features_clf])\n",
    "    flag_probs_xgb = final_classifier_xb.predict_proba(test_df[features_clf])\n",
    "    flag_probs = 0.7 * flag_probs_lgb + 0.3 * flag_probs_xgb\n",
    "    \n",
    "    flag_preds_mapped = np.argmax(flag_probs, axis=1)\n",
    "    \n",
    "    # Regressor: average raw predictions, then calibrate\n",
    "    capacity_preds = np.zeros(len(test_df))\n",
    "    event_indices = np.where(flag_preds_mapped != 1)[0]  # where flag != 0\n",
    "    \n",
    "    if len(event_indices) > 0:\n",
    "        X_test_events = test_df.iloc[event_indices][features_reg]\n",
    "        lgb_preds = final_regressor.predict(X_test_events)\n",
    "        xgb_preds = final_regressor_xb.predict(X_test_events)\n",
    "        ensemble_raw = 0.7 * lgb_preds + 0.3 * xgb_preds\n",
    "        capacity_preds[event_indices] = ensemble_raw\n",
    "    \n",
    "    return {'flag_probs': flag_probs, 'capacity': capacity_preds}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Main Execution - Global Model\n",
    "Process the full dataset for the global model, generate features, and train/predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T15:36:33.691828Z",
     "iopub.status.busy": "2025-11-02T15:36:33.691633Z",
     "iopub.status.idle": "2025-11-02T15:46:31.679448Z",
     "shell.execute_reply": "2025-11-02T15:46:31.678638Z",
     "shell.execute_reply.started": "2025-11-02T15:36:33.691813Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "--- STEP 1: PROCESSING GLOBAL MODEL ---\n",
      "============================================================\n",
      "  - Stage 1: Creating baseline and full feature sets...\n",
      "  - Stage 2: Selecting stable baseline features via correlation...\n",
      "    Stable baseline feature count: 45\n",
      "  - Stage 3: Building specialized feature lists for models...\n",
      "  - Global model using 50 clf features and 58 reg features.\n",
      "  - Starting training pipeline...\n",
      "    Skipping GridSearchCV. Using pre-defined best parameters.\n",
      "    Using CLF Params: {'colsample_bytree': 0.8, 'learning_rate': 0.026, 'min_child_samples': 20, 'n_estimators': 2500, 'num_leaves': 29, 'reg_alpha': 0.46, 'reg_lambda': 0.5}\n",
      "    Using REG Params: {'colsample_bytree': 0.8, 'learning_rate': 0.0175, 'min_child_samples': 20, 'n_estimators': 10000, 'num_leaves': 49, 'reg_alpha': 0.3, 'reg_lambda': 0.2}\n",
      "    Training final models on full data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Generating ensemble predictions...\n"
     ]
    }
   ],
   "source": [
    "# --- GLOBAL MODEL ---\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n--- STEP 1: PROCESSING GLOBAL MODEL ---\\n\" + \"=\"*60)\n",
    "# Generate features on the full dataset\n",
    "train_df_processed, test_df_processed, features_clf_global, features_reg_global = generate_and_select_features(train_df, test_df)\n",
    "print(f\"  - Global model using {len(features_clf_global)} clf features and {len(features_reg_global)} reg features.\")\n",
    "\n",
    "global_params = {\n",
    "    \"CLF\" : {'colsample_bytree': 0.8, 'learning_rate': 0.026, 'min_child_samples': 20, 'n_estimators': 2500, 'num_leaves': 29, 'reg_alpha': 0.46, 'reg_lambda': 0.5},\n",
    "    \"REG\" : {'colsample_bytree': 0.8, 'learning_rate': 0.0175, 'min_child_samples': 20, 'n_estimators': 10000, 'num_leaves': 49, 'reg_alpha': 0.3, 'reg_lambda': 0.2}\n",
    "}\n",
    "\n",
    "selected_features = ['month', 'temp_roll_max_96', 'power_pct_change_4', 'temp_diff_24',\n",
    "'power_lag_24', 'temp_lag_96', 'archetype_small_temp_interaction',\n",
    "'power_roll_mean_24', 'power_lag_8', 'temp_roll_min_24', 'dayofweek',\n",
    "'is_heating_likely', 'temp_x_hour_sin', 'is_morning_ramp_up', 'power_diff_1', 'is_peak_hours', 'state_changed',\n",
    "'cooling_degree', 'power_diff_96', 'temp_ewm_span_24', 'temp_roll_std_8',\n",
    "'power_ewm_span_8', 'temp_x_is_weekend', 'power_vs_roll_mean_96', 'temp_roll_std_4',\n",
    "'temp_lag_144', 'is_business_hours', 'power_pct_change_96', 'temp_rad_interaction',\n",
    "'archetype_large', 'power_vs_daily_max', 'is_weekend', 'is_summer', 'is_mild_temp_workday',\n",
    "'archetype_small_power_interaction', 'Global_Horizontal_Radiation_W/m2', 'Building_Power_kW',\n",
    "'power_roll_std_4', 'power_roll_mean_96', 'is_winter', 'archetype_large_power_interaction', \n",
    "'quarter', 'power_lag_48']\n",
    "\n",
    "official = ['power_roll_mean_4', 'power_cumsum_4', 'is_lunch_hour', 'temp_bins_cold', 'time_of_day_minute', \n",
    "            'time_of_day_fraction', 'hour_sin', 'is_evening_ramp_down', 'is_last_work_hour', 'power_roll_max_4',\n",
    "            'power_roll_max_12', 'power_roll_max_24', 'power_roll_max_48', 'power_diff_24', \n",
    "            'power_zscore_24', 'power_zscore_96', 'power_percentile_96']\n",
    "\n",
    "forgetten = ['hour_cos', 'power_roll_median_24', \"power_roll_min_48\", \"power_roll_median_48\",\n",
    "\"power_roll_min_96\", \"power_roll_max_96\", \"power_roll_min_144\", \"power_roll_median_144\",\n",
    "\"power_roll_skew_24\", \"power_roll_kurt_24\", \"power_roll_kurt_96\", \"power_pct_change_1\",\n",
    "\"power_diff_4\", \"power_pct_change_24\", \"power_diff_672\", \"power_pct_change_672\", \"is_work_hours_lag\", \"time_since_last_transition\",\n",
    "\"state_Weekend_Day\", \"state_Workday_Hours\", \"temp_x_is_work_hours\", \"rad_x_is_work_hours\", \"rad_x_is_weekend\", \n",
    "\"rad_x_hour_cos\", \"effective_temp_load\", \"hour_x_power_roll_mean_24\", \"power_lag_96_x_dayofweek\", \"rad_x_power_lag_96\",\n",
    "\"temp_x_power_roll_mean_12\", \"rad_roll_sum_12\", \"temp_roll_sum_24\", \"rad_roll_sum_24\", \"power_roll_sum_48\", \n",
    "\"rad_roll_sum_48\", \"power_vs_daily_avg\", \"power_zscore_24\", \"power_zscore_96\", \"power_percentile_96\"]\n",
    "\n",
    "features_reg_global = list(set(selected_features + official + forgetten))\n",
    "\n",
    "\n",
    "global_predictions = train_and_predict_pipeline(train_df_processed, test_df_processed, features_clf_global, features_reg_global, run_grid_search=RUN_GRID_SEARCH, params=global_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Main Execution - Site-Specific (Archetype) Models\n",
    "Train models per site archetype ('small' and 'large') and collect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T15:46:31.681062Z",
     "iopub.status.busy": "2025-11-02T15:46:31.680797Z",
     "iopub.status.idle": "2025-11-02T15:56:57.951549Z",
     "shell.execute_reply": "2025-11-02T15:56:57.950984Z",
     "shell.execute_reply.started": "2025-11-02T15:46:31.681044Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "--- STEP 2: PROCESSING SITE-SPECIFIC MODELS ---\n",
      "============================================================\n",
      "{'siteA': 'small', 'siteB': 'small', 'siteC': 'large', 'siteD': 'small', 'siteE': 'small', 'siteF': 'large', 'siteG': 'small', 'siteH': 'small', 'siteI': 'large', 'siteJ': 'large', 'siteK': 'large', 'siteL': 'large', 'siteM': 'large'}\n",
      "\n",
      "--- Processing Archetype-Specific Model for: 'small' ---\n",
      "  - Stage 1: Creating baseline and full feature sets...\n",
      "  - Stage 2: Selecting stable baseline features via correlation...\n",
      "    Stable baseline feature count: 52\n",
      "  - Stage 3: Building specialized feature lists for models...\n",
      "  - Archetype model using 56 clf features and 65 reg features.\n",
      "  - Starting training pipeline...\n",
      "    Skipping GridSearchCV. Using pre-defined best parameters.\n",
      "    Using CLF Params: {'colsample_bytree': 0.8, 'learning_rate': 0.026, 'min_child_samples': 20, 'n_estimators': 2500, 'num_leaves': 29, 'reg_alpha': 0.46, 'reg_lambda': 0.5}\n",
      "    Using REG Params: {'colsample_bytree': 0.8, 'learning_rate': 0.0175, 'min_child_samples': 20, 'n_estimators': 10000, 'num_leaves': 49, 'reg_alpha': 0.3, 'reg_lambda': 0.2}\n",
      "    Training final models on full data...\n",
      "    Generating ensemble predictions...\n",
      "\n",
      "--- Processing Archetype-Specific Model for: 'large' ---\n",
      "  - Stage 1: Creating baseline and full feature sets...\n",
      "  - Stage 2: Selecting stable baseline features via correlation...\n",
      "    Stable baseline feature count: 52\n",
      "  - Stage 3: Building specialized feature lists for models...\n",
      "  - Archetype model using 56 clf features and 65 reg features.\n",
      "  - Starting training pipeline...\n",
      "    Skipping GridSearchCV. Using pre-defined best parameters.\n",
      "    Using CLF Params: {'colsample_bytree': 0.8, 'learning_rate': 0.026, 'min_child_samples': 20, 'n_estimators': 2500, 'num_leaves': 29, 'reg_alpha': 0.46, 'reg_lambda': 0.5}\n",
      "    Using REG Params: {'colsample_bytree': 0.8, 'learning_rate': 0.0175, 'min_child_samples': 20, 'n_estimators': 10000, 'num_leaves': 49, 'reg_alpha': 0.3, 'reg_lambda': 0.2}\n",
      "    Training final models on full data...\n",
      "    Generating ensemble predictions...\n"
     ]
    }
   ],
   "source": [
    "# --- SITE-SPECIFIC MODELS ---\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n--- STEP 2: PROCESSING SITE-SPECIFIC MODELS ---\\n\" + \"=\"*60)\n",
    "\n",
    "# MODIFIED: Use a single, robust parameter set for all site models.\n",
    "archetype_params = {\n",
    "    \"CLF\" : {'colsample_bytree': 0.8, 'learning_rate': 0.026, 'min_child_samples': 20, 'n_estimators': 2500, 'num_leaves': 29, 'reg_alpha': 0.46, 'reg_lambda': 0.5},\n",
    "    \"REG\" : {'colsample_bytree': 0.8, 'learning_rate': 0.0175, 'min_child_samples': 20, 'n_estimators': 10000, 'num_leaves': 49, 'reg_alpha': 0.3, 'reg_lambda': 0.2}\n",
    "}\n",
    "\n",
    "\n",
    "all_archetype_predictions = []\n",
    "# Loop over each unique archetype ('small', 'large')\n",
    "print(SITE_ARCHETYPE_MAP)\n",
    "for archetype in ['small', 'large']:\n",
    "    print(f\"\\n--- Processing Archetype-Specific Model for: '{archetype}' ---\")\n",
    "\n",
    "    # Find all sites that belong to the current archetype\n",
    "    sites_in_archetype = [site for site, arch in SITE_ARCHETYPE_MAP.items() if arch == archetype]\n",
    "    \n",
    "    # Filter the ORIGINAL raw dataframes for this group of sites\n",
    "    train_arch_raw = train_df[train_df[COL_SITE].isin(sites_in_archetype)].copy()\n",
    "    test_arch_raw = test_df[test_df[COL_SITE].isin(sites_in_archetype)].copy()\n",
    "\n",
    "    if len(train_arch_raw) == 0 or len(test_arch_raw) == 0:\n",
    "        print(f\"  - WARNING: No data for archetype {archetype}. Skipping.\")\n",
    "        continue\n",
    "        \n",
    "    # Run feature engineering and selection specifically for this group of sites\n",
    "    train_df_arch, test_df_arch, features_clf_arch, features_reg_arch = generate_and_select_features(train_arch_raw, test_arch_raw)\n",
    "    \n",
    "    print(f\"  - Archetype model using {len(features_clf_arch)} clf features and {len(features_reg_arch)} reg features.\")\n",
    "    \n",
    "    # Train a single model on all data for this archetype\n",
    "    arch_predictions = train_and_predict_pipeline(\n",
    "        train_df_arch,\n",
    "        test_df_arch,\n",
    "        features_clf_arch,\n",
    "        features_reg_global,\n",
    "        run_grid_search=RUN_GRID_SEARCH,\n",
    "        params=archetype_params\n",
    "    )\n",
    "    \n",
    "    # Store the predictions for all sites in this archetype\n",
    "    arch_pred_df = pd.DataFrame({\n",
    "        'Site': test_df_arch[COL_SITE],\n",
    "        'Timestamp_Local': test_df_arch[COL_TIME],\n",
    "        'flag_probs_arch': list(arch_predictions['flag_probs']),\n",
    "        'capacity_arch': arch_predictions['capacity']\n",
    "    })\n",
    "    all_archetype_predictions.append(arch_pred_df)\n",
    "\n",
    "\n",
    "archetype_predictions_df = pd.concat(all_archetype_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ensembling & Final Submission\n",
    "Combine global and archetype predictions with weighted averaging, apply post-processing, and save to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T15:56:57.952700Z",
     "iopub.status.busy": "2025-11-02T15:56:57.952405Z",
     "iopub.status.idle": "2025-11-02T15:57:03.448719Z",
     "shell.execute_reply": "2025-11-02T15:57:03.447885Z",
     "shell.execute_reply.started": "2025-11-02T15:56:57.952675Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "--- STEP 3: ENSEMBLING & SAVING ---\n",
      "============================================================\n",
      "\n",
      "Ensembled submission file 'submission.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Ensembling & Final Submission ---\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n--- STEP 3: ENSEMBLING & SAVING ---\\n\" + \"=\"*60)\n",
    "global_predictions_df = pd.DataFrame({\n",
    "    'Site': test_df_processed[COL_SITE], \n",
    "    'Timestamp_Local': test_df_processed[COL_TIME], \n",
    "    'flag_probs_global': list(global_predictions['flag_probs']), \n",
    "    'capacity_global': global_predictions['capacity']\n",
    "})\n",
    "\n",
    "# Merge global predictions with the site-specific ones\n",
    "ensembled_df = pd.merge(global_predictions_df, archetype_predictions_df, on=[COL_SITE, COL_TIME], how='left')\n",
    "\n",
    "# Handle cases where a site-specific model might have failed (e.g., no training data)\n",
    "ensembled_df['flag_probs_arch'] = ensembled_df['flag_probs_arch'].fillna(ensembled_df['flag_probs_global'])\n",
    "ensembled_df['capacity_arch'] = ensembled_df['capacity_arch'].fillna(ensembled_df['capacity_global'])\n",
    "\n",
    "# MODIFIED: Your ensembling weights. These are hyperparameters you can tune.\n",
    "# A 90/10 split is a good starting point.\n",
    "global_weight = 0.9\n",
    "site_weight = 0.1\n",
    "ensembled_df['flag_probs_ensembled'] = ensembled_df.apply(lambda r: (np.array(r['flag_probs_global']) * global_weight + np.array(r['flag_probs_arch']) * site_weight), axis=1)\n",
    "ensembled_df['capacity_ensembled'] = (ensembled_df['capacity_global'] * global_weight) + (ensembled_df['capacity_arch'] * site_weight)\n",
    "\n",
    "# Create final submission columns\n",
    "ensembled_df['flag_mapped_ensembled'] = ensembled_df['flag_probs_ensembled'].apply(np.argmax)\n",
    "reverse_flag_mapping = {0: -1, 1: 0, 2: 1}\n",
    "ensembled_df[COL_FLAG] = ensembled_df['flag_mapped_ensembled'].map(reverse_flag_mapping)\n",
    "ensembled_df[COL_CAP] = ensembled_df['capacity_ensembled']\n",
    "\n",
    "# Post-processing: ensure capacity is 0 when flag is 0\n",
    "ensembled_df.loc[ensembled_df[COL_FLAG] == 0, COL_CAP] = 0\n",
    "\n",
    "# 1. Define the columns required for the submission\n",
    "submission_columns = [COL_SITE, COL_TIME, COL_FLAG, COL_CAP]\n",
    "\n",
    "# 2. Select only these columns and sort them to ensure correct order\n",
    "final_submission = ensembled_df[submission_columns].sort_values(by=[COL_SITE, COL_TIME])\n",
    "\n",
    "# 3. Save the final DataFrame to CSV\n",
    "final_submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nEnsembled submission file 'submission.csv' created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook complete!"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8392895,
     "sourceId": 13448092,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
